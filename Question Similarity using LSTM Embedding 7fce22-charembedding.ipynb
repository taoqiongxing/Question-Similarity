{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/Ax/Desktop/chars/train.csv', encoding='utf-8')\n",
    "df_train['id'] = df_train['id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('C:/Users/Ax/Desktop/chars/test.csv', encoding='utf-8')\n",
    "df_test['test_id'] = df_test['test_id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat((df_train, df_test),sort=True)\n",
    "df_all['question1'].fillna('', inplace=True)\n",
    "df_all['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1=df_all['question1']\n",
    "question2=df_all['question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.6B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    vocab.append('unk') #装载不认识的词\n",
    "    embd.append([0]*300) #这个emb_size可能需要指定\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'W:0' shape=(3049, 300) dtype=float32_ref>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.5464599 ,  2.2850914 , -3.084309  , ..., -0.5284521 ,\n",
       "        -0.34052843, -2.0187469 ],\n",
       "       [-9.0163555 , -3.8010836 , -7.210567  , ..., -2.6530488 ,\n",
       "         1.8687314 ,  2.1470642 ],\n",
       "       ...,\n",
       "       [-1.6323291 , -0.24604906,  0.21140376, ..., -1.264535  ,\n",
       "         3.3599977 ,  0.23747765],\n",
       "       [ 1.6436294 , -5.375357  ,  5.102956  , ..., -4.8079195 ,\n",
       "        -0.36651626,  2.1845698 ],\n",
       "       [ 1.5330207 , -0.89149183,  0.7716491 , ..., -2.4461007 ,\n",
       "         5.5994077 , -2.099302  ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-6f0f1c560f22>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(20)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "input_x1 = np.array(list(vocab_processor.transform(question1)))\n",
    "input_x2 = np.array(list(vocab_processor.transform(question2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor at 0x27d6d45f908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1=tf.nn.embedding_lookup(W, input_x1)\n",
    "question2=tf.nn.embedding_lookup(W, input_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(427342, 20, 300) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all['question1']=question1\n",
    "#df_all['question2']=question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.concat((df_train, df_test),sort=True)\n",
    "#df_all['question1'].fillna('', inplace=True)\n",
    "#df_all['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
    "of pandas will change to not sort by default.\n",
    "\n",
    "To accept the future behavior, pass 'sort=True'.\n",
    "\n",
    "To retain the current behavior and silence the warning, pass sort=False\n",
    "\n",
    "  \"\"\"Entry point for launching an IPython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_vectorizer = CountVectorizer(max_features=10000-1).fit(\n",
    "    itertools.chain(df_all['question1'], df_all['question2']))\n",
    "other_index = len(counts_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenizer = re.compile(counts_vectorizer.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, max_len=20):\n",
    "    seqs = texts.apply(lambda s: \n",
    "        [counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n",
    "         for w in words_tokenizer.findall(s.lower())])\n",
    "    return pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all.sample(1000) # Just for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-505ebce7c35c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'question1' is not defined"
     ]
    }
   ],
   "source": [
    "#df_all['question1']=question1\n",
    "#df_all['question2']=question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.6B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    vocab.append('unk') #装载不认识的词\n",
    "    embd.append([0]*300) #这个emb_size可能需要指定\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.5464599 ,  2.2850914 , -3.084309  , ..., -0.5284521 ,\n",
       "        -0.34052843, -2.0187469 ],\n",
       "       [-9.0163555 , -3.8010836 , -7.210567  , ..., -2.6530488 ,\n",
       "         1.8687314 ,  2.1470642 ],\n",
       "       ...,\n",
       "       [-1.6323291 , -0.24604906,  0.21140376, ..., -1.264535  ,\n",
       "         3.3599977 ,  0.23747765],\n",
       "       [ 1.6436294 , -5.375357  ,  5.102956  , ..., -4.8079195 ,\n",
       "        -0.36651626,  2.1845698 ],\n",
       "       [ 1.5330207 , -0.89149183,  0.7716491 , ..., -2.4461007 ,\n",
       "         5.5994077 , -2.099302  ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1960, 1440, 1552],\n",
       "       [   0,    0,    0, ...,   93, 1225, 1556],\n",
       "       [   0,    0,    0, ..., 2275,  930,  324],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  127,  495,  395],\n",
       "       [   0,    0,    0, ..., 1684,  509,  377],\n",
       "       [   0,    0,    0, ...,   29, 1879, 1210]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a980d8c3ce15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpretrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#transform inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minput_x1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    201\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mid\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \"\"\"\n\u001b[1;32m--> 203\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m       \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_document_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtokenizer\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \"\"\"\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32myield\u001b[0m \u001b[0mTOKENIZER_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(30)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "input_x1 = np.array(list(vocab_processor.transform(X1_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x1 = np.array(list(vocab_processor.transform(X1_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as lyr\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0, 2667, 1452, 1659, 1093, 1981, 1684, 2009, 1189])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train[(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 20, 300)      808500      input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          570368      embedding_5[0][0]                \n",
      "                                                                 embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 256)          0           lstm_5[0][0]                     \n",
      "                                                                 lstm_5[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           8224        multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            33          dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,387,125\n",
      "Trainable params: 1,387,125\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1_tensor = lyr.Input(X1_train.shape[1:])\n",
    "input2_tensor = lyr.Input(X2_train.shape[1:])\n",
    "\n",
    "words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 300)\n",
    "seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n",
    "\n",
    "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
    "\n",
    "merge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
    "\n",
    "dense1_layer = lyr.Dense(32, activation='sigmoid')(merge_layer)\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\n",
    "\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228947"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(254386, 20, 300) dtype=float32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/10\n",
      " - 160s - loss: 0.4350 - val_loss: 0.3377\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, \n",
    "          validation_data=([X1_val, X2_val], y_val), \n",
    "          batch_size=64, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "features_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train = features_model.predict([X1_train, X2_train], batch_size=64)\n",
    "F_val = features_model.predict([X1_val, X2_val], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain = xgb.DMatrix(F_train, label=y_train)\n",
    "dVal = xgb.DMatrix(F_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.660504\tval-logloss:0.662923\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.43992\tval-logloss:0.467727\n",
      "[20]\ttrain-logloss:0.327839\tval-logloss:0.37369\n",
      "[30]\ttrain-logloss:0.264958\tval-logloss:0.324712\n",
      "[40]\ttrain-logloss:0.220948\tval-logloss:0.293513\n",
      "[50]\ttrain-logloss:0.187915\tval-logloss:0.271873\n",
      "[60]\ttrain-logloss:0.162466\tval-logloss:0.256672\n",
      "[70]\ttrain-logloss:0.144503\tval-logloss:0.247411\n",
      "[80]\ttrain-logloss:0.129654\tval-logloss:0.240586\n",
      "[90]\ttrain-logloss:0.117203\tval-logloss:0.23526\n",
      "[100]\ttrain-logloss:0.106697\tval-logloss:0.231831\n",
      "[110]\ttrain-logloss:0.097804\tval-logloss:0.229468\n",
      "[120]\ttrain-logloss:0.090606\tval-logloss:0.228007\n",
      "[130]\ttrain-logloss:0.083734\tval-logloss:0.226698\n",
      "[140]\ttrain-logloss:0.077725\tval-logloss:0.226177\n",
      "[150]\ttrain-logloss:0.072251\tval-logloss:0.225875\n",
      "[160]\ttrain-logloss:0.067387\tval-logloss:0.225909\n",
      "Stopping. Best iteration:\n",
      "[157]\ttrain-logloss:0.068735\tval-logloss:0.22566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question1'])\n",
    "X2_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_test = features_model.predict([X1_test, X2_test], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTest = xgb.DMatrix(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "        'test_id': df_all[df_all['test_id'].notnull()]['test_id'].values,\n",
    "        'y_pre': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.539208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_pre\n",
       "test_id          \n",
       "0        0.054278\n",
       "1        0.978075\n",
       "2        0.240083\n",
       "3        0.998168\n",
       "4        0.539208"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26d52808c88>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub['y_pre'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission20180624LSTM_06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
